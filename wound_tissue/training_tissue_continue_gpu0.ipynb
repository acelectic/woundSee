{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version = 3.7.6\n",
      "Tensorflow Version = 1.15.0\n",
      "GPU Available:  True\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(1, '../..')\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "# \"\" = CPU\n",
    "# \"0\" = GPU_0\n",
    "# \"1\" = GPU_1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; \n",
    "# print( 'CUDA Version =', os.environ['CUDA_VERSION'] )\n",
    "\n",
    "import sys\n",
    "print( 'Python Version =', sys.version.split()[0] )\n",
    "\n",
    "import tensorflow as tf\n",
    "print( 'Tensorflow Version =',tf.__version__)\n",
    "\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from os.path import join\n",
    "from utils_1 import *\n",
    "import shutil\n",
    "import gc\n",
    "from inspect import getsource\n",
    "import time\n",
    "\n",
    "# import wandb\n",
    "# from wandb.tensorflow import WandbHook\n",
    "# from wandb.keras import WandbCallback\n",
    "\n",
    "# wandb.init(project=\"woundsee\", sync_tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x( paths ):\n",
    "    def get_x_rgb( path ):\n",
    "        rgb = read_image( path, 'rgb' )\n",
    "        return rgb / 255\n",
    "\n",
    "    get_x = get_x_rgb\n",
    "    x = np.array( [ get_x(path) for path in paths ] ).astype( np.float32 )\n",
    "    return x\n",
    "\n",
    "def load_y( paths ):\n",
    "    def get_y( path ):\n",
    "        grayscale = read_image( path, 'gray' ) / 63\n",
    "        layers = [  (grayscale==i) + [0] for i in range(0, 5) ]\n",
    "        \n",
    "        return np.stack( layers, axis=2 )\n",
    "\n",
    "    y = np.array( [ get_y(path) for path in paths ] ).astype( np.float32 )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator( this_fold_dir, t, color ):\n",
    "    def my_generator( samples, batch_size, color ):\n",
    "        L = len( samples )\n",
    "        while True:\n",
    "            batch_start = 0\n",
    "            batch_end = batch_size\n",
    "            shuffle(samples)\n",
    "            f_paths = [ os.path.join( feature_dir, i ) for i in samples ]\n",
    "            l_paths = [ os.path.join( label_dir, i ) for i in samples ]\n",
    "            while batch_start < L:\n",
    "                limit = min( batch_end, L )\n",
    "                x = load_x( f_paths[batch_start:limit] )\n",
    "                y = load_y( l_paths[batch_start:limit] )\n",
    "                yield x, y\n",
    "                batch_start += batch_size\n",
    "                batch_end += batch_size\n",
    "    \n",
    "#     if not (t == \"Train\" or t == \"Validate\"):\n",
    "#         raise ValueError('t must be Train or Validate')\n",
    "    \n",
    "    my_dir = os.path.join( this_fold_dir, t )\n",
    "    feature_dir = os.path.join( my_dir, 'Features' )\n",
    "    label_dir = os.path.join( my_dir, 'Labels' )\n",
    "    \n",
    "    \n",
    "    samples = [ p for p in os.listdir( feature_dir ) if 'checkpoint' not in p]\n",
    "    \n",
    "    if isTest:\n",
    "        samples = samples[:256]\n",
    "    \n",
    "    genarator = my_generator( samples, batch_size, color )\n",
    "    return genarator, len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( fold ):\n",
    "    \n",
    "    this_ex_dir = os.path.join( this_experiment_dir, str(fold) )\n",
    "    \n",
    "    if not isTest:\n",
    "        os.mkdir( this_ex_dir )\n",
    "    \n",
    "    this_fold_dir = os.path.join( dataset, str(fold) )\n",
    "    \n",
    "    print( \"Start Process fold: %d\" % (fold) )\n",
    "    train_gen, num_train_samples = get_generator( this_fold_dir, \"Train\", 'rgb' )\n",
    "    val_gen, num_val_samples = get_generator( this_fold_dir, \"Validate\", 'rgb' )\n",
    "#     val_gen, num_val_samples = get_generator( '/notebooks/VOLUME_1TB/Thesis_dataset', \"Test_Sub-Images\", 'rgb' )\n",
    "    \n",
    "    print( \"Train: %d, Val: %d\" % ( num_train_samples, num_val_samples ) )\n",
    "\n",
    "    \n",
    "    callbacks = get_call_backs(this_ex_dir)\n",
    "#     model     = get_model( input_height, input_width, input_chanel )\n",
    "    model     = get_model( model_path )\n",
    "    \n",
    "    if isTest:\n",
    "        model.summary()\n",
    "    \n",
    "    train_history = model.fit_generator( \n",
    "        generator = train_gen\n",
    "        ,steps_per_epoch = np.ceil( num_train_samples / batch_size )\n",
    "        ,validation_data = val_gen\n",
    "        ,validation_steps = np.ceil( num_val_samples / batch_size )\n",
    "        ,epochs=epochs\n",
    "        ,callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    if not isTest:\n",
    "        np.save( os.path.join( this_ex_dir, 'history') , np.array(train_history.history))\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensorboard_callback( this_ex_dir ):\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "    path = os.path.join( os.path.join(this_ex_dir, 'tensorboard') )\n",
    "    os.mkdir(path)\n",
    "    print( path )\n",
    "    tbCallBack = TensorBoard(log_dir=path,profile_batch=0, histogram_freq=0, write_graph=True, write_images=True)\n",
    "    return tbCallBack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adaptive_lr_callback():\n",
    "    from tensorflow.keras.callbacks import LearningRateScheduler    \n",
    "    adaptive_lr_callback=LearningRateScheduler(lr_scheduler)\n",
    "    return adaptive_lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_call_backs(this_ex_dir):\n",
    "    \n",
    "    adaptive_lr = get_adaptive_lr_callback()\n",
    "    callbacks = [ adaptive_lr ]\n",
    "    \n",
    "    if not isTest:\n",
    "        log_cb = get_log_callback(this_ex_dir)\n",
    "        callbacks.append( log_cb )\n",
    "        \n",
    "        tensorboard = get_tensorboard_callback( this_ex_dir )\n",
    "        callbacks.append( tensorboard )\n",
    "        \n",
    "#         callbacks.append(WandbCallback())\n",
    "        \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_experiment():\n",
    "    def create_experiment_dir():\n",
    "        os.mkdir( this_experiment_dir )\n",
    "        print(this_experiment_dir)\n",
    "        \n",
    "    def create_detail_of_experiment():\n",
    "        detail_path = os.path.join( this_experiment_dir, 'detail.txt' )\n",
    "        with open( detail_path, 'w', encoding=\"utf-8\" ) as file:\n",
    "            pass\n",
    "        return detail_path\n",
    "    \n",
    "    def write_line_to_detail( msg ):\n",
    "        with open( detail_path, 'a', encoding=\"utf-8\" ) as file:\n",
    "            file.write( \"%s\\n\\n\" % msg )\n",
    "            \n",
    "    def write_model_to_detail( model ):\n",
    "        with open( detail_path, 'a', encoding=\"utf-8\" ) as file:\n",
    "            model.summary(print_fn=lambda x: file.write(x + '\\n'))\n",
    "            \n",
    "    model = get_model(model_path)\n",
    "    \n",
    "    create_experiment_dir()\n",
    "    detail_path = create_detail_of_experiment()\n",
    "    \n",
    "    write_line_to_detail( 'Method: %s' % method )\n",
    "    write_line_to_detail( 'Structure: %s' % structure )\n",
    "    write_line_to_detail( 'Input Shape: %s' % str( ( input_height, input_width, input_chanel) ) )\n",
    "    write_line_to_detail( 'Color: %s' % color )\n",
    "    write_line_to_detail( 'Description: %s' % desc )\n",
    "    write_line_to_detail( 'Batch size: %d' % batch_size )\n",
    "    write_line_to_detail( 'Epochs: %d' % epochs )\n",
    "    write_line_to_detail( 'Start lr: %s' % learning_rate)\n",
    "    \n",
    "    adaptive = getsource( lr_scheduler )\n",
    "    write_line_to_detail( adaptive )\n",
    "    \n",
    "    loss_func = getsource( loss )\n",
    "    write_line_to_detail( loss_func )\n",
    "    \n",
    "    write_model_to_detail( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model( input_height, input_width, input_chanel ):\n",
    "    \n",
    "#     _dict = {\n",
    "#         'u-net-res-2': get_model_unet_res_2,\n",
    "#     }\n",
    "    \n",
    "#     if structure not in _dict.keys():\n",
    "#         raise ValueError('%s not in our model.' % structure)\n",
    "    \n",
    "#     return _dict[structure]( input_height, input_width, input_chanel )\n",
    "\n",
    "\n",
    "def get_model(model_path):\n",
    "    from tensorflow.keras.models import load_model\n",
    "    return load_model( model_path ,custom_objects={ 'loss_func':loss, 'loss': loss, 'dice':dice , 'iou': iou, 'cate': cate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start( start, end  ):\n",
    "    \n",
    "    def start_train():\n",
    "        for i in range(start,end):\n",
    "            print(\"_\" * 100)\n",
    "            gc.collect()\n",
    "            train(i)\n",
    "            print(\"_\" * 100)\n",
    "        gc.collect()\n",
    "    \n",
    "    def calculate_environment_name():\n",
    "        global this_experiment_dir\n",
    "        number_folder       = len(os.listdir(Experiments))\n",
    "        this_experiment_dir = os.path.join( Experiments, str( number_folder ).zfill(4) )\n",
    "        print( \"Experiments: %s\" % str( number_folder ).zfill(4) )\n",
    "        \n",
    "    calculate_environment_name()\n",
    "    \n",
    "    if isTest:\n",
    "        start_train()\n",
    "        return\n",
    "    \n",
    "    setup_experiment()\n",
    "    start_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_callback(this_ex_dir, log_name = 'log.txt', prefix_model=''):\n",
    "    log_path = os.path.join( this_ex_dir, log_name)\n",
    "    from tensorflow.keras.callbacks import Callback\n",
    "    from tensorflow.keras import backend as K\n",
    "    \n",
    "    class My_Callback_1(Callback):\n",
    "        def __init__(self, filename, separator=' ; '):\n",
    "            self.sep = separator\n",
    "            self.filename = filename\n",
    "            self.val_a = -1\n",
    "            self.best_epoch = -1\n",
    "            super(My_Callback_1, self).__init__()\n",
    "        \n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.tic = time.time()\n",
    "        \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            toc = time.time()\n",
    "            t_sec = round( toc - self.tic)\n",
    "            \n",
    "            lr_var = self.model.optimizer.lr\n",
    "            lr = K.get_value(lr_var)\n",
    "            \n",
    "            logs = logs or {}\n",
    "            val = [ \"%4d\" % epoch, \"%7d\" % t_sec, \"%10s\" % lr ]\n",
    "            for key in sorted( logs.keys() ):\n",
    "                if key == 'lr':\n",
    "                    continue\n",
    "                value = logs[key]\n",
    "                val.append( \"%3.6f\" % value )\n",
    "            line = self.sep.join( val )\n",
    "            with open( self.filename, 'a' ) as f:\n",
    "                f.write( \"\\n%s\" % line )\n",
    "            \n",
    "            if epoch+1 >= 50 and (epoch+1) % 10 == 0:\n",
    "                model_name = \"%s%d_model.h5\" % (prefix_model,epoch+1)\n",
    "                model_path = os.path.join( this_ex_dir, model_name )\n",
    "                self.model.save( model_path )\n",
    "                \n",
    "                \n",
    "            val_n = logs[ \"val_dice\" ]\n",
    "            if val_n >= self.val_a:\n",
    "                self.val_a = val_n\n",
    "                self.best_epoch = epoch\n",
    "                model_name = \"%sbest_model.h5\" % (prefix_model)\n",
    "                model_path = os.path.join( this_ex_dir, model_name )\n",
    "                self.model.save( model_path )\n",
    "                with open( self.filename, 'a' ) as f:\n",
    "                    f.write( \" ***\" )\n",
    "                \n",
    "                \n",
    "        def on_train_begin(self, logs=None):\n",
    "            line = \"epochs\" + self.sep + \"secs\" + self.sep + \"lr\" + self.sep + self.sep.join( sorted(self.params['metrics']) )\n",
    "            with open( self.filename, 'w' ) as f:\n",
    "                f.write( \"%s\" % line )\n",
    "        \n",
    "        def on_train_end(self, logs=None):\n",
    "            model_name = \"%send_model.h5\" % prefix_model\n",
    "            model_path = os.path.join( this_ex_dir, model_name )\n",
    "            self.model.save( model_path )\n",
    "            with open( self.filename, 'a' ) as f:\n",
    "                f.write( \"\\n\\nBest Epoch: %d, val__dice: %.6f\" % (self.best_epoch,self.val_a) )\n",
    "    \n",
    "    my_callback = My_Callback_1(log_path)\n",
    "    \n",
    "    return my_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_unet_res_2( input_height, input_width, input_chanel ):\n",
    "#     from tensorflow.keras.layers import ZeroPadding2D, Input, Conv2D, MaxPooling2D, SpatialDropout2D, concatenate, Conv2DTranspose, BatchNormalization, Activation, Add\n",
    "#     from tensorflow.keras.models import Model\n",
    "#     from tensorflow.keras.optimizers import Adam\n",
    "#     from tensorflow.keras.activations import relu, softmax\n",
    "    \n",
    "#     def residual_block( filters, kernel_size ):\n",
    "#         def block( X ):\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X_shortcut = X\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X = BatchNormalization() ( X )\n",
    "#             X = Activation( relu ) (X)\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X = Add()( [ X_shortcut, X ] )\n",
    "#             X = BatchNormalization() ( X )\n",
    "#             X = Activation( relu ) (X)\n",
    "#             X_shortcut = X\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X = BatchNormalization() ( X )\n",
    "#             X = Activation( relu ) (X)\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X = Add()( [ X_shortcut, X ] )\n",
    "#             X = BatchNormalization() ( X )\n",
    "#             X = Activation( relu ) (X)\n",
    "#             return X\n",
    "#         return block\n",
    "    \n",
    "#     def residual_block_2( filters, kernel_size ):\n",
    "#         def block( X ):\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X_shortcut = X\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X = Activation( relu ) (X)\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X = Add()( [ X_shortcut, X ] )\n",
    "#             X = Activation( relu ) (X)\n",
    "#             X_shortcut = X\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X = Activation( relu ) (X)\n",
    "#             X = Conv2D( filters=filters, kernel_size=kernel_size, padding='same' ) ( X )\n",
    "#             X = Add()( [ X_shortcut, X ] )\n",
    "#             X = Activation( relu ) (X)\n",
    "#             return X\n",
    "            \n",
    "#         return block\n",
    "    \n",
    "#     # INPUT\n",
    "#     input_shape = ( input_height, input_width, input_chanel )\n",
    "#     input_      = Input( input_shape )\n",
    "    \n",
    "#     X = Conv2D( filters=16, kernel_size=(7,7), padding='same' ) ( input_ )\n",
    "    \n",
    "#     encode_1 = residual_block( filters=32, kernel_size=(3, 3) ) ( X )\n",
    "#     pool_1 = MaxPooling2D( pool_size=(2,2) ) ( encode_1 )\n",
    "    \n",
    "#     encode_2 = residual_block( filters=64, kernel_size=(3, 3) ) ( pool_1 )\n",
    "#     pool_2 = MaxPooling2D( pool_size=(2,2) ) ( encode_2 )\n",
    "    \n",
    "#     encode_3 = residual_block( filters=128, kernel_size=(3, 3) ) ( pool_2 )\n",
    "#     pool_3 = MaxPooling2D( pool_size=(2,2) ) ( encode_3 )\n",
    "    \n",
    "#     mid_1 = residual_block( filters=256, kernel_size=(3, 3) ) (pool_3)\n",
    "    \n",
    "#     decode_3 = Conv2DTranspose( filters=128, kernel_size=(3, 3), strides=(2, 2), padding='same') (mid_1)\n",
    "#     decode_3 = concatenate([ encode_3, decode_3 ], axis=-1)\n",
    "#     decode_3 = residual_block_2( filters=128, kernel_size=(3, 3) ) ( decode_3 )\n",
    "#     decode_3 = SpatialDropout2D( 0.1 ) (decode_3)\n",
    "    \n",
    "#     decode_2 = Conv2DTranspose( filters=64, kernel_size=(3, 3), strides=(2, 2), padding='same') (decode_3)\n",
    "#     decode_2 = concatenate([ encode_2, decode_2 ], axis=-1)\n",
    "#     decode_2 = residual_block_2( filters=64, kernel_size=(3, 3) ) ( decode_2 )\n",
    "#     decode_2 = SpatialDropout2D( 0.1 ) (decode_2)\n",
    "    \n",
    "#     decode_1 = Conv2DTranspose( filters=32, kernel_size=(3, 3), strides=(2, 2), padding='same') (decode_2)\n",
    "#     decode_1 = concatenate([ encode_1, decode_1 ], axis=-1)\n",
    "#     decode_1 = residual_block_2( filters=32, kernel_size=(3, 3) ) ( decode_1 )\n",
    "    \n",
    "#     output = SpatialDropout2D( 0.2 ) ( decode_1 )\n",
    "#     output = Conv2D(filters=5, kernel_size=(1, 1), padding='same' ) ( output )\n",
    "#     output = Activation( softmax ) (output)\n",
    "    \n",
    "#     model = Model( inputs=[input_], outputs=[output], name='U-Net with ResNet Block' )\n",
    "#     optimizer = Adam( lr=learning_rate )\n",
    "#     model.compile( optimizer=optimizer, loss=loss, metrics=[dice, iou, cate] )\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred):\n",
    "    from tensorflow.keras import backend as K\n",
    "    smooth = 1\n",
    "    y_pred = y_pred[...,1:]\n",
    "    y_true = y_true[...,1:]\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n",
    "    return ( intersection + smooth ) / ( union + smooth )\n",
    "    \n",
    "\n",
    "def dice(y_true, y_pred ):\n",
    "    from tensorflow.keras import backend as K\n",
    "    smooth = 1\n",
    "    y_pred = y_pred[...,1:]\n",
    "    y_true = y_true[...,1:]\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    denom = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    return (2. * intersection + smooth) / (denom + smooth)\n",
    "    \n",
    "\n",
    "def cate(y_true, y_pred ):\n",
    "    from tensorflow.keras.metrics import categorical_accuracy\n",
    "    return categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import backend as K\n",
    "    \n",
    "    def dice(y_true, y_pred):\n",
    "        y_pred = y_pred[...,1:]\n",
    "        y_true = y_true[...,1:]\n",
    "        intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "        denom = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "        d = (2. * intersection + smooth) / (denom + smooth)\n",
    "        return d\n",
    "    \n",
    "    def iou(y_true, y_pred):\n",
    "        intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "        union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n",
    "        return ( intersection + smooth ) / ( union + smooth )\n",
    "    \n",
    "    smooth = 1\n",
    "    d = K.pow(1-dice(y_true, y_pred), 2)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "        decay_rate = .5\n",
    "        decay_step = 25\n",
    "        if epoch % decay_step == 0 and epoch >= 50:\n",
    "            return lr * decay_rate\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "method        = 'Sub-images'\n",
    "isTest        = False\n",
    "epochs        = 140\n",
    "batch_size    = 6\n",
    "learning_rate = 1e-04\n",
    "color         = 'rgb'\n",
    "structure     = 'u-net-res-2'\n",
    "\n",
    "height = 512\n",
    "width  = 512\n",
    "# stride = 128\n",
    "\n",
    "\n",
    "input_height, input_width, input_chanel = ( height, width, 3 )\n",
    "1\n",
    "dataset_name        = \"wound_rajavithi_korean_medetec\"\n",
    "k_fold              = 10\n",
    "\n",
    "root                = join(\"../../..\", \"data\", dataset_name, \"wound_segmentation\", \"wound_tissue\")\n",
    "\n",
    "model_path          = os.path.join(\"best_model_rajavithi_korean.h5\")\n",
    "\n",
    "\n",
    "\n",
    "source_dir          = join(root, \"training_k_fold_with_rotation_color\")\n",
    "\n",
    "Experiments         = join(root, \"experiments\")\n",
    "\n",
    "if not is_exists( Experiments ):\n",
    "    os.mkdir( Experiments )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/wound_rajavithi_korean_medetec/wound_segmentation/wound_tissue/training_k_fold_with_rotation_color\n",
      "Experiments: 0004\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "../../../data/wound_rajavithi_korean_medetec/wound_segmentation/wound_tissue/experiments/0004\n",
      "____________________________________________________________________________________________________\n",
      "Start Process fold: 2\n",
      "Train: 14767, Val: 1641\n",
      "../../../data/wound_rajavithi_korean_medetec/wound_segmentation/wound_tissue/experiments/0004/2/tensorboard\n",
      "Epoch 1/140\n",
      "2461/2462 [============================>.] - ETA: 0s - loss: 0.0760 - dice: 0.7656 - iou: 0.6397 - cate: 0.8806Epoch 1/140\n",
      "2462/2462 [==============================] - 1739s 706ms/step - loss: 0.0760 - dice: 0.7656 - iou: 0.6397 - cate: 0.8806 - val_loss: 0.1173 - val_dice: 0.7015 - val_iou: 0.5649 - val_cate: 0.8478\n",
      "Epoch 2/140\n",
      "2461/2462 [============================>.] - ETA: 0s - loss: 0.0500 - dice: 0.8087 - iou: 0.6924 - cate: 0.9013Epoch 1/140\n",
      "2462/2462 [==============================] - 1710s 694ms/step - loss: 0.0500 - dice: 0.8087 - iou: 0.6924 - cate: 0.9013 - val_loss: 0.1343 - val_dice: 0.6773 - val_iou: 0.5367 - val_cate: 0.8401\n",
      "Epoch 3/140\n",
      "2461/2462 [============================>.] - ETA: 0s - loss: 0.0402 - dice: 0.8275 - iou: 0.7166 - cate: 0.9106Epoch 1/140\n",
      "2462/2462 [==============================] - 1716s 697ms/step - loss: 0.0402 - dice: 0.8275 - iou: 0.7166 - cate: 0.9105 - val_loss: 0.1295 - val_dice: 0.6844 - val_iou: 0.5446 - val_cate: 0.8456\n",
      "Epoch 4/140\n",
      "2461/2462 [============================>.] - ETA: 0s - loss: 0.0340 - dice: 0.8403 - iou: 0.7336 - cate: 0.9170Epoch 1/140\n",
      "2462/2462 [==============================] - 1720s 699ms/step - loss: 0.0340 - dice: 0.8403 - iou: 0.7336 - cate: 0.9170 - val_loss: 0.1245 - val_dice: 0.6895 - val_iou: 0.5501 - val_cate: 0.8434\n",
      "Epoch 5/140\n",
      "2461/2462 [============================>.] - ETA: 0s - loss: 0.0297 - dice: 0.8504 - iou: 0.7476 - cate: 0.9225Epoch 1/140\n",
      "2462/2462 [==============================] - 1717s 697ms/step - loss: 0.0297 - dice: 0.8504 - iou: 0.7476 - cate: 0.9225 - val_loss: 0.1262 - val_dice: 0.6886 - val_iou: 0.5490 - val_cate: 0.8467\n",
      "Epoch 6/140\n",
      "2462/2462 [==============================] - 1718s 698ms/step - loss: 0.0261 - dice: 0.8594 - iou: 0.7602 - cate: 0.9274 - val_loss: 0.1423 - val_dice: 0.6649 - val_iou: 0.5221 - val_cate: 0.8384\n",
      "Epoch 7/140\n",
      "2462/2462 [==============================] - 1710s 694ms/step - loss: 0.0232 - dice: 0.8680 - iou: 0.7728 - cate: 0.9319 - val_loss: 0.1298 - val_dice: 0.6747 - val_iou: 0.5286 - val_cate: 0.8373\n",
      "Epoch 8/140\n",
      "2461/2462 [============================>.] - ETA: 0s - loss: 0.0211 - dice: 0.8745 - iou: 0.7825 - cate: 0.9355Epoch 1/140\n",
      "2462/2462 [==============================] - 1717s 698ms/step - loss: 0.0211 - dice: 0.8745 - iou: 0.7825 - cate: 0.9355 - val_loss: 0.1335 - val_dice: 0.6730 - val_iou: 0.5282 - val_cate: 0.8443\n",
      "Epoch 9/140\n",
      "2461/2462 [============================>.] - ETA: 0s - loss: 0.0206 - dice: 0.8767 - iou: 0.7861 - cate: 0.9370Epoch 1/140\n",
      "2462/2462 [==============================] - 1713s 696ms/step - loss: 0.0206 - dice: 0.8767 - iou: 0.7861 - cate: 0.9370 - val_loss: 0.1272 - val_dice: 0.6838 - val_iou: 0.5417 - val_cate: 0.8508\n",
      "Epoch 10/140\n",
      " 356/2462 [===>..........................] - ETA: 22:48 - loss: 0.0185 - dice: 0.8832 - iou: 0.7960 - cate: 0.9393"
     ]
    }
   ],
   "source": [
    "if not is_exists(model_path):\n",
    "    print(\"can't found model:\", model_path)\n",
    "    \n",
    "else :\n",
    "    for p in [ source_dir ]:\n",
    "        global dataset, desc, this_experiment_dir \n",
    "        desc    = p\n",
    "        dataset = p\n",
    "\n",
    "        print(p)\n",
    "        start( 2, 4+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
